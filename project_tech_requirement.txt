
ğŸ› ï¸ Development Stack
Language: Python 3.11 (Anaconda)

Dependencies:

torch + transformers

bitsandbytes (quantization)

sounddevice / PyAudio

openai-whisper / faster-whisper

bark / tts

numpy, yaml, argparse

Optional: langchain if planning structured chain expansions

Hardware: Linux, NVIDIA GPU 4â€“8 GB VRAM


ğŸ“¦ File structure

voice_assistant/
â”‚
â”œâ”€â”€ main.py               # Entry point
â”œâ”€â”€ requirements.txt      # Dependencies for pip/conda
â”œâ”€â”€ config.yaml           # Model paths, device config
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ audio_io.py       # Microphone capture + speaker playback
â”‚   â”œâ”€â”€ stt.py            # Whisper transcription logic
â”‚   â”œâ”€â”€ llm.py            # LLM inference wrapper
â”‚   â”œâ”€â”€ tts.py            # Bark or TTS inference
â”‚   â””â”€â”€ controller.py     # Pipeline orchestration
â”‚
â””â”€â”€ utils/
    â””â”€â”€ logger.py         # Logging helper


ğŸ“¦ Universal architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Microphoneâ”‚â”€Audioâ†’ â”‚  STT Model â”‚â”€Textâ†’ â”‚     LLM      â”‚â”€Textâ†’ â”‚   TTS Model â”‚â”€Audioâ†’ Speaker
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸ“¦ MVP Modules
1ï¸âƒ£ Audio I/O Module

Uses sounddevice or PyAudio for capturing PCM audio chunks

Handles:

Hotkey/hotword activation or manual press-to-talk

Stop recording after silence or max duration

Playback audio (wav) for output

2ï¸âƒ£ Speech-to-Text (STT) Module

Uses Whisper (base.en or small.en)

GPU accelerated if possible

Outputs raw transcribed text

3ï¸âƒ£ LLM Processing Module

Load quantized LLaMA2 or TinyLLaMA in 4-bit using:

transformers + bitsandbytes

or llama.cpp / ollama for easier integration

Prompt:

System message

User transcribed text

Maintains short context (last 1â€“3 turns only)

4ï¸âƒ£ Text-to-Speech (TTS) Module

Uses Bark for realistic speech generation

Generates WAV or PCM waveform for immediate playback

Simple TTS fallback if GPU VRAM is tight

5ï¸âƒ£ Controller / Pipeline Orchestrator

Coordinates:

Capture â†’ Transcribe â†’ Generate â†’ Synthesize â†’ Playback

Can use async/event loop or threading for non-blocking I/O

CLI interface for initial development/debugging



ğŸª„ Agile Development Plan
Sprint 1 (Core Pipeline Prototype)
âœ… Audio capture + playback working
âœ… Transcribe single utterance to text with Whisper
âœ… Generate a simple text reply using local LLM (quantized)
âœ… Print generated text for verification

Sprint 2 (Integrate Speech Output)
âœ… Add Bark/Coqui TTS
âœ… Synthesize generated text to audio
âœ… Playback assistantâ€™s speech response
âœ… Enable single-turn voice-to-voice conversation

Sprint 3 (Refinements)
âœ… Loop for continuous conversation
âœ… Context buffer (last 1â€“3 exchanges)
âœ… Add optional web search for â€œwho/whatâ€ queries (offline fallback)
âœ… CLI flags for debug/verbose/logging modes
âœ… Test latency and optimize for <3â€“4 sec response time


ğŸš€ Next Steps
âœ… Confirm you are ready to begin coding, starting with Sprint 1.
âœ… Let me know if you want me to generate the main.py scaffold + module stubs for you to start iterating immediately within your repo.
âœ… After Sprint 1, we will plan MVP test cases and optimization (quantization tuning, latency tests) for stability before moving to file system and reminder features.